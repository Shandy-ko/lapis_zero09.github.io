Title: 意味表現学習まわり
Date: 2017-3-13 22:00
Category: python, study, word2vec, CBOW, SG, GloVe, ppmi, svd
Tags: NLP
Slug: 29th
Author: lapis_zero09
Summary: CBOW, SG, GloVeについて

# CBOW SG GloVe

連続単語袋詰めモデル(continuous bag-of-words model，CBOW)
と
連続スキップグラムモデル(continuous skip-gram model，SG)
は
Mikolovらによって提案された単語の分散的意味表現手法．

これらのモデルを公開しているツール→word2vec


## CBOW

与えられた文脈の中で出現している文脈語を使って，ある対象語が出現しているかどうかを予測可能な意味表現を学習．

対象語 意味を表現したい単語
文脈語 ある単語の周辺に現れる単語

文s = 「私はご飯と味噌汁を食べた．」について，

形態素は，[私，は，ご飯，と，味噌汁，を，食べた]

対象語を「ご飯」とすると，それ以外の語が文脈語として用い，「ご飯」の出現を予測．

CBOWモデルでは，文章中に[私，は，と，味噌汁，を，食べた]という文脈語が出現していた時に，
その文章中に「ご飯」という単語が出現するかどうか予測できるように，
それぞれの単語の意味表現ベクトルを更新することが目的．

1つの単語につき，d次元のベクトル

i番目の単語が対象語の場合，その単語の対象語ベクトル$\boldsymbol{x}_i$を使用．

i番目の単語が文脈語の場合，その単語の文脈語ベクトル$\boldsymbol{z}_i$を使用．

対象語xが文脈中のi番目の単語して出現している場合，xを中心とする(2k+1)個の単語からなる文脈
$(i-k), \cdots, (i-1), i, (i+1), \cdots, (i+k)$
を使って予測する問題を考える．

この文脈中に出現する文脈語についての文脈ベクトルを
$\boldsymbol{z}_{i-k}, \cdots, \boldsymbol{z}_{i-1}, \boldsymbol{z}_{i+1}, \cdots, \boldsymbol{z}_{i+k}$
とすると対象語の出現確率は，
$p(\boldsymbol{x}_i|\boldsymbol{z}_{i-k}, \cdots, \boldsymbol{z}_{i-1}, \boldsymbol{z}_{i+1}, \cdots, \boldsymbol{z}_{i+k})$
と表すことができる．


→kを大きくするとより広い範囲で単語の共起が考慮可能．関係が低い離れた単語同士の共起も考慮されてしまう

→kはCBOWのハイパーパラメータ．Mikolovらは$k=2$として5単語からなる文脈窓を用いて学習

問題:長い連続する単語列の出現は大きなコーパスについても少ない→その出現確率を推定することは難しい

CBOWにおける解決策:文脈語の出現順序を無視，下式で与えられる文脈語ベクトルの平均ベクトル$\hat{\boldsymbol{x}}$を，対象語xの文脈を代表するベクトルとして用いる．

$$
\hat{\boldsymbol{x}} = \frac{1}{2k}(\boldsymbol{z}_{i-k} + \cdots + \boldsymbol{z}_{i-1} + \boldsymbol{z}_{i+1} + \cdots + \boldsymbol{z}_{i+k})
$$

$\boldsymbol{x}$と$\hat{\boldsymbol{x}}$を用いて，対象語の出現確率をソフトマックス関数で以下のように計算

$$
p(\boldsymbol{x}_i|\boldsymbol{z}_{i-k}, \cdots, \boldsymbol{z}_{i-1}, \boldsymbol{z}_{i+1}, \cdots, \boldsymbol{z}_{i+k}) = \frac{\exp(\hat{\boldsymbol{x}}^{\mathrm{T}} \boldsymbol{x})}{\sum_{x' \in \boldsymbol{\nu}}\exp(\hat{\boldsymbol{x}}^{\mathrm{T}} \boldsymbol{x'})}
$$

$\boldsymbol{\nu}$はコーパス中の全単語からなる語彙集合，$\boldsymbol{x'}$は$\boldsymbol{\nu}$中の単語を表す

xが文章中に出現する可能性が高いほど右辺・分子の内積の値が大きくする

1つの単語について2つのベクトルが付与される(文脈語ベクトルと対象語ベクトル)→単語の共起をベクトルの内積で表現しているから

1つじゃダメなの？→1つの場合，$\hat{\boldsymbol{x}}$の中にもxが現れることになり，$\boldsymbol{x}^{\mathrm{T}} x$を小さくすることになる．

→xの長さ(l2ノルム)を小さくすることになる

→xのl2ノルムを変えてもソフトマックス関数はスケール不変

→xをそのl2ノルムで割って正規化しても
$p(x_i|\boldsymbol{z}_{i-k}, \cdots, \boldsymbol{z}_{i-1}, \boldsymbol{z}_{i+1}, \cdots, \boldsymbol{z}_{i+k})$
の値は変わらない

→異なる2つのベクトルを使うことでこの問題を解決

tips:単語の共起頻度を離散的な数ではなく，連続な値(ベクトルの内積)でモデル化しているので「continuous」

上記では出現順序を無視し，文脈語の平均を使って文脈を表現したが，出現順序を保つベクトルを作成することも可能

→文脈語のベクトルを連結する


## SG

対象語を使って，文脈に出現している文脈語を予測する


対象語の「ご飯」を持ちいて，他の単語([私，は，と，味噌汁，を，食べた])の出現を予測

対象語xがi番目の単語として出現している文脈で他の単語を同時に予測する場合の確率を下式で計算．

$$
p(z_{i-k}, \cdots, z_{i-1}, z_{i+1}, \cdots, z{i+k}|x) = p(z_{i-k}|x)\cdots p(z_{i-1}|x)p(z_{i+1}|x)\cdots p(z_{i+k}|x)
$$

※対象語xが与えられているとき，文脈語が全て独立

独立性の仮定により，
$$
p(z|x) = \frac{\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})}{\sum_{z' \in \boldsymbol{\nu} (x)}\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z'})}
$$

(→「わかりやすいパターン認識」など参照のこと)

$\boldsymbol{\nu} (x)$は対象語xが出現している文脈中での文脈語の集合．ある特定の文脈でxと共起する文脈語の集合ではなく，コーパス全体におけるxが出現する全ての文脈に関しての文脈語の集合

CBOWと同様文脈として，対象語xを中心とするk個の単語からなる文脈窓を選ぶことも可能

→kを固定せずに，それぞれのxの出現においてkとしてある区間内のランダム値を取るなど，文脈窓の幅を動的に決めることも可能

n個の単語それぞれについて，d次元の対象語ベクトルを学習

それぞれの単語について何らかの文脈で共起する文脈語が$\boldsymbol{\nu} (x)$なので，その単語全てについてd次元の文脈語ベクトルを学習


---

CBOWはSGに比べ，文脈語の独立性を近似していないため，より正確

1対1の共起をモデル化しているSGに比べ，CBOWは1対複数の文脈語をモデル化しているので，複数の文脈語からなる単語列がコーパス中に十分な回数現れなければならない

→CBOWはより大きなコーパスが必要


## CBOWとSGのモデル最適化

SGのモデルについて，

$$
p(z|x) = \frac{\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})}{\sum_{z' \in \boldsymbol{\nu} (x)}\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z'})}
$$

xまたはzについて凸関数ではない→局所解が存在

xのある要素に注目するとxとzの内積はその要素に対して線形な関数となる

xかzのどちらかを固定した場合，片方の関数について上式は凸関数となる→双線形関数，交互最適化可能

さらに，上式は\expを含むので対数をとることで対数双線形関数

xかzを固定すると片方の変数に関してソフトマックス関数→多クラスロジスティック回帰としてみなせる



## 負例サンプリング

SGのモデルについて，単語の分散的意味表現を学習するために，
ある単語と共起する単語(正例)だけでなく，
共起しない単語(負例)に関する情報も必要

→共起しない単語集合は膨大．その中から学習のために「良い負例」を選択しなければいけない

→この負例手法のことを負例サンプリング(Negative Sampling)

### SGのNegative Sampling

ある単語xの分散的意味表現$\boldsymbol{x}$を求める問題を考える

正例として扱える文脈語の集合を$\boldsymbol{D}_{pos} = \boldsymbol{\nu} (x)$とし，負例の集合を$\boldsymbol{D}_{neg} \in \boldsymbol{\nu}$とする

それぞれの定義より，$\boldsymbol{D}_{pos} \cup \boldsymbol{D}_{neg} = \phi$

この時，SGモデルで定義される共起の予測確率に従い，
対数尤度を最大化するxの対象語ベクトルは下式で与えられる

$$
\hat{\boldsymbol{x}} = argmax_{x} (\sum_{z \in \boldsymbol{D}_{pos}} \log(p(t=1|\boldsymbol{x} ,\boldsymbol{z})p_{pos}(z|x)) + \sum_{z \in \boldsymbol{D}_{neg}} \log(p(t=-1|\boldsymbol{x} ,\boldsymbol{z})p_{neg}(z|x)))
$$


右辺第1項は対象語xと共起する文脈語zに関する対数尤度を表す．
その項の$t=1$は正例であることを示す．

$p_{pos}(z|x)$はxの正例集合$\boldsymbol{D}_{pos}$での出現確率を表しており，
正例集合はコーパス中に出現している単語集合$\boldsymbol{\nu} (x)$なので，
$\sum_{z \in \boldsymbol{D}_{pos}} p_{pos}(z|x) = 1$となる

右辺第2項について，負例$\boldsymbol{\nu}$はランダムにサンプリングしているためサンプリング手法に依存する．

本来なら負例は正例を除いた$\boldsymbol{\nu}$からサンプリングすべきだが，$\boldsymbol{\nu}$に比べ，$\boldsymbol{D}_{pos}$は極めて小さいため近似的に$\boldsymbol{D}_{neg}$を$\boldsymbol{\nu}$からサンプリングしている

→どの対象語xについても同一分布が使える利点

サンプリング手法は？

→SGではコーパス中で高出現確率である単語zが単語xの意味表現学習の負例として選ばれるように，単語zのユニグラム分布$p(z)$に基づいて負例集合を選択

負例が(x, z)のペアで決まるのなら，ユニグラム分布ではなく，xとzの同時分布からサンプリングすべきでは？

→2単語の同時共起確率に比べ，1単語の出現確率がゼロになりにくいという利点．

→膨大なコーパスから共起を計算するには大きな共起表を作らなければならず，空間計算量という点で好ましくない


単語の出現はZipfの法則に従うので$p(z)$はロングテールの分布となる．

→$p(z)$を3/4乗した値に比例する分布をサンプリング分布として用いている．

→出現確率$p(z)$が小さい文脈語zも比較的高頻度でサンプリングされるようになる

以上より，


$$
argmax_{x} (\sum_{z \in \boldsymbol{D}_{pos}} \log(p(t=1|\boldsymbol{x} ,\boldsymbol{z})) + \sum_{z \in \boldsymbol{D}_{neg}} \log((1 - p(t=1|\boldsymbol{x} ,\boldsymbol{z}))\tilde{p}(z))) = argmax_{x} (\sum_{z \in \boldsymbol{D}_{pos}} \log(\sigma (\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})) + \sum_{z \in \boldsymbol{D}_{neg}} \log(\sigma (-\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})\tilde{p}(z)))
$$

$\sigma(\theta)$はロジスティックシグモイド関数

第2項の負例に関するサンプリング分布$\tilde{p}(z)$はxに無関係．
よって，第2項を期待値の形で書き表すことが可能

$$
\sum_{z \in \boldsymbol{D}_{pos}} \log(\sigma (\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})) + E_{\tilde{p}(z)} [ \log(\sigma (-\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})) ]
$$


1つの正例に対して，k個の負例を用いる．

負例はランダムに選択している擬似負例のため正例数に比べ，負例数を大きくしなければならない．

実用的には$k=20$．



word2vecに実装されているCBOWとSGでは，
非同期パラメータ更新による並列処理を行うことで学習時間の短縮

→複数のスレッドを用いて，同一学習モデルを更新していく．

→同じ学習モデルを更新すると互いに更新する値を打ち消しあう可能性，必ずしも正しく学習が行えるわけではない

より正確にするには

→オンライン学習の並列化でよく用いられるように反復的パラメータ混合方

→スレッドごとに独立にモデルを持たせて，独立的に学習させたモデルを足し合わせる


学習の正確さ トレードオフ 大量のデータから短時間で学習



## 階層型ソフトマックスによる近似計算

大規模なコーパスについて，多数の対象語と文脈語に関してベクトルを計算しなければならず，
高速に計算できることが望まれる．

$$
p(z|x) = \frac{\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})}{\sum_{z' \in \boldsymbol{\nu} (x)}\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z'})}
$$

右辺分母は全文脈語彙集合にわたって規格化しなければならず，この計算に時間がかかる

解決策

→単語の階層構造を事前に事前に用意し，それに従って規格化を行う→階層型ソフトマックス

→単語の出現を考慮する代わりに，その単語を含むグループを考慮できるので，
式中の和の計算を行う際に，考慮すべき語彙集合を小さくできる

ある単語xがある1つのグループにのみ含まれるようにグループ分けされていれば，
全単語集合ではなく，このグループ内でのみxの確率を規格化しておけば良い

言語モデル構築の際に，単語そのものの出現頻度の代わりに，単語グループの出現頻度を使うことで出現頻度が少ない単語の出現確率を求める手法から発想を得ている



単語の階層構造をどのようにして作るか

1. 階層的クラスタリングで全ての単語を含む階層構造を作成
2. すでに構築されている概念構造を使う
  -  WordNet→単語を語義ごとにグループ化
    - グループ間で上位下位関係が記述されている



階層的クラスタリングとWordNetを使って大規模なコーパスから単語の分散的意味表現を学習する際の問題点

- 階層的クラスタリングの場合

  1. 単語と単語間の類似度とグループとグループ間の類似度を計算しなければならないため，計算時間がかかる

  2. どのような類似度尺度を使うべきかが明確ではない
    - 階層的クラスタリングによって作られる階層構造が，類似度尺度によって変わる

- WordNetの場合

  - WordNetに登録されていない単語をどのように分類するか


word2vecではハフマン木を使うことで単語の階層構造を構築

→単語の出現頻度を計算し，出現頻度の高い順にソート

→階層的クラスタリングよりも計算量の点で有利

→コーパス中の全ての単語がハフマン木のノードで表されているので未知語の問題が起きない



単語の階層構造を用いて，どのようにして確率$P(z|x)$を計算するか

ハフマン木上で単語zに対する頂点を見つけ，根からその頂点への経路Path(z)を求める

各ノードでは-1もしくは1という2つの枝があるため，$p(z|x)$の予測確率をロジスティック関数を使って，次のように近似できる

\begin{eqnarray*}
p(z|x) &=& \frac{\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z})}{\sum_{z' \in \boldsymbol{\nu} (x)}\exp(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{z'})}\\
& \approx & \prod_{(l,y) \in Path(z)} p(t=l|\boldsymbol{x},\boldsymbol{y})\\
&=& \prod_{(l,y) \in Path(z)} \sigma(l\boldsymbol{x}^{\mathrm{T}} \boldsymbol{y})
\end{eqnarray*}

yは根から単語zに対する葉への経路上のノードを表し，lは各ノードでの枝の値(1, -1)を表し，
このlに対応する確率変数がt

ハフマン木は単語の出現頻度だけを使って構築されているので，
単語の意味的関係を反映しているとは言えないが元の式を近似するには十分な階層構造である．



## 大域ベクトル予測モデル

CBOWとSGはコーパスを一文単位で処理し，単語毎の2つのベクトルを更新している

→単語の共起を予測する際に，一文に含まれている情報しか使うことができない

→分散的意味表現ベクトルを学習する際に一文中の共起共起情報ではなく，
分布的意味表現のように，コーパス全体における2つの単語共起情報を使う手法

→大域ベクトル予測モデル(global vector prediction，GloVe)

分布的意味表現のように，コーパス全体における対象語と文脈語のコーパス全体における共起頻度を計算し，
共起行列$\boldsymbol{x}$を作成

$\boldsymbol{x}$の各行が対象語・各列が文脈語に対応しているとする

→i番目の対象語iとj番目の文脈語jが共起する確率回数を$\boldsymbol{x}$の(i,j)番目の要素$\boldsymbol{x}_{ij}$

次の目的関数を最小化する対象語ベクトル$\boldsymbol{x}_i$と文脈語ベクトル$\boldsymbol{z}_j$を学習

$$
J = \sum_{i,j=1}^{|\boldsymbol{\nu}|}f(X_{ij})(\boldsymbol{x}_i^{\mathrm{T}}\boldsymbol{z}_j + b_i + b_j - \log(X_{ij}))^2
$$

$b_i$と$b_j$はスカラーのバイアス項

→対象語ベクトルと文脈語ベクトルの内積を用いて，この共起頻度の対数を予測

関数fは次の3つの性質を満たす関数

1. $f(0)=0$
  - $x \to 0$のとき$\lim_{x \to 0} f(x)\log^2(x)$が有限の値を持つ．
    - 共起頻度が0であるペアに関してもJが無限大に発散してしまうのを防ぐ

2. $f(x)$は単調増加関数．
  - 低共起頻度のペアを過剰評価するのを防ぐ

3. $f(x)$はある閾値以上の共起頻度に関しては比較的小さな値を持つ
  - 不要語など高頻出する単語との共起を過剰評価するのを防ぐ

GloVeは$f$に下式を採用

$$
f(t) = \begin{cases}
  (t/t_{max})^\alpha & (t < t_{max}) \\
  1 & (otherwise)
\end{cases}
$$

Jは$\boldsymbol{x}_i$あるいは$\boldsymbol{z}_j$のどちらか一方を固定させた場合，
片方に対して線形であり，双線形関数．

→交互最適化手法



---

- GloVe → 目的関数が二乗誤差
- CBOW SG → 交差エントロピー誤差


GloVeがCBOW SGと大きく異なる点→負例を必要としない

→負例サンプリングを行うための様々な仮定や近似が不要になる

n個の単語の共起情報を保存するためには{n(n-1)/2 - n}個の変数(自分自身の共起は計算しないので-n)が必要となる

→空間情報量としては$O(n^2)$のオーダーの変数が必要

→共起行列をどのように計算するかがGloVeを用いて学習するための重要な前処理タスク

→実際どうするか

1. 分散処理
2. コーパス全体における各単語の出現頻度を求め，頻度が小さいものは共起行列に含まないようにする．
  - 出現頻度の低いもの→スペルミスの可能性
  - 出現頻度が低いと共起の可能性も低い→信頼できる統計情報が得られない可能性
  - 共起行列を構築するために最低2回コーパスを処理しなければならない

→CBOWとSGは一文単位で学習できるのでオンライン学習が可能



## 意味表現の評価

分散的意味表現において，ベクトルの各次元がどのような意味に対応しているか分かっていない

→学習された意味表現を別のタスクに応用し，
その応用先タスクにおける精度がどれくらい出るかで正確さを評価

→間接的評価方法

→評価対象を直接評価→直接的評価方法

間接的評価方法

→CBOWやSGでは分散的意味表現を学習するのにある文脈において2つの単語が共起するかを予測している

→共起が正確に予測できているかで評価可能

単語の意味表現の正確さを評価するために用いられるタスク

1. 2つの単語間の意味類似性を予測するタスク
2. 2つの単語ペア間の関係類似性を予測するタスク


### 意味的類似性予測タスク

単語同士の意味がどれくらい似ているか
(意味的類似性(semantic similarity))を求めることができれば，
意味が近い単語同士に高い類似性スコアが計算できるかどうかで
意味表現そのものの正確さを間接的に評価可能

意味的類似性は類義性(synonymy)，関連性(relatedness)よりも広義な概念

例：「暑い」と「寒い」という対義語について
温度という属性は共通しているので意味的類似性が高い単語ペア


## 使ってみる

上記のCBOW，SG，GloVeに加え，分布的意味表現を日本語記事に対して適用してみる．

対象は[livedoorニュースコーパス](http://www.rondhuit.com/download.html#ldcc)

コードはここ([https://github.com/lapis-zero09/compare_word_embedding](https://github.com/lapis-zero09/compare_word_embedding))に

全記事に対して，preprocess.pyで形態素解析を行い，gensimで処理可能な形にする．

解析器にはMeCabを用いて，辞書はipadic-nelogd

glove_pythonの導入に戸惑ったMacユーザは以下のREADMEを参照すると良いかも

[https://github.com/lapis-zero09/compare_word_embedding](https://github.com/lapis-zero09/compare_word_embedding)

全ての形態素の原型を取り出し，一記事を一行にまとめる．

それぞれの学習は，


```
$ python w2v.py
$ python glove_train.py
$ python ppmi.py
$ python svd.py
```


それぞれイテレーション10，windowサイズ10，次元指定できるものは100次元で学習させた結果が以下

いずれもiphoneの類語を予測


```
CBOW_with_hs
[('iphone4', 0.515411376953125), ('webブラウザ', 0.4230906665325165), ('ios6', 0.4147755801677704), ('ipad', 0.4116036593914032), ('ipad2', 0.38188278675079346), ('アップル', 0.37905681133270264), ('iphone4s', 0.367786169052124), ('ソフトバンクbb', 0.36410054564476013), ('タンブラー', 0.36315712332725525), ('大丈夫', 0.35265758633613586)]

CBOW_with_ns15
[('iphone4', 0.6266778707504272), ('ipad', 0.6240962743759155), ('touch', 0.6063281297683716), ('ipod', 0.5750889778137207), ('ios', 0.5344790816307068), ('4s', 0.5055159330368042), ('アップル', 0.49820417165756226), ('ipad2', 0.4975413680076599), ('防水ケース', 0.4778713583946228), ('kobo', 0.47309401631355286)]

CBOW_with_hs_ns15
[('iphone4', 0.5731303691864014), ('iphone4s', 0.4358748197555542), ('ios', 0.42091381549835205), ('ipad2', 0.41879040002822876), ('アップル', 0.41285741329193115), ('防水ケース', 0.3713395595550537), ('ios6', 0.3705442547798157), ('ソフトバンクbb', 0.3699251115322113), ('ホカホンhd', 0.3648505210876465), ('コネクター', 0.36398276686668396)]

SG_with_hs
[('4s', 0.7601545453071594), ('ipod', 0.7516292333602905), ('ipad', 0.7158170938491821), ('touch', 0.7114615440368652), ('ios', 0.6660245656967163), ('第4世代', 0.6656962037086487), ('3gs', 0.6580607891082764), ('iphone4', 0.6171021461486816), ('配布', 0.5599908828735352), ('互換', 0.551994264125824)]

SG_with_ns15
[('ipad', 0.7440428733825684), ('4s', 0.741912305355072), ('ipod', 0.7315280437469482), ('3gs', 0.7070977091789246), ('touch', 0.672669529914856), ('ios', 0.6507753729820251), ('iphone4', 0.6480903625488281), ('第4世代', 0.6351419687271118), ('for', 0.6325072050094604), ('フリーペーパー', 0.5929508209228516)]

SG_with_hs_ns15
[('ipod', 0.7355147004127502), ('4s', 0.7283318042755127), ('ipad', 0.7088928818702698), ('touch', 0.6909142732620239), ('3gs', 0.6866996884346008), ('ios', 0.6676512956619263), ('iphone4', 0.6078430414199829), ('フリーペーパー', 0.6007919907569885), ('配布', 0.6005358695983887), ('第4世代', 0.6001726388931274)]

glove
[('ipad', 1.7425963151785258), ('アプリ', 1.4982636197187946), ('4s', 1.4977384448379325), ('ipod', 1.4966988373925263), ('4', 1.4776524801798216), ('touch', 1.3836936937733773), ('話題', 1.2830962460190283), ('使える', 1.267703173083845), ('用', 1.2650386013010513), ('向け', 1.26418467742026)]

ppmi
[('ipod', 0.2859661921087914), ('ipad', 0.24883286298039248), ('4s', 0.2447535806470899), ('touch', 0.2349337206345609), ('第4世代', 0.22949601240870882), ('3gs', 0.18854934562128067), ('4', 0.15223633417263555), ('ios', 0.14752415366324956), ('第3世代', 0.14382973978764851), ('据え置く', 0.13690344251866945)]

svd
[('売れ筋', 0.07950519361803438), ('touch', 0.07289990333758399), ('ガイガーカウンター', 0.0720546914668142), ('ipod', 0.06996932998910954), ('お浚い', 0.06655078247840501), ('ホルダー', 0.06373112962844232), ('電子書籍', 0.06339235714445059), ('スタンド', 0.062353061811765656), ('計測', 0.06080525440724401), ('ゲーム機', 0.06062771603307594)]
```


パラメータ最適化とかは以下の論文を参照のこと

- [Improving Distributional Similarity with Lessons Learned from Word Embeddings](https://transacl.org/ojs/index.php/tacl/article/view/570)


参照

- ダヌシカ・ボレガラ．ウェブデータの機械学習．講談社，2016．p. 192．
- [glove-python](https://github.com/maciejkula/glove-python)
- [Making Sense of Word2vec](https://github.com/piskvorky/word_embeddings)
