Title: 意味表現学習まわり
Date: 2017-2-5 22:00
Category: python, study, word2vec, CBOW, SG, GloVe
Tags: NLP
Slug: 29th
Author: lapis_zero09
Summary: CBOW, SG, GloVeについて

# CBOW SG GloVe

連続単語袋詰めモデル(continuous bag-of-words model，CBOW)
と
連続スキップグラムモデル(continuous skip-gram model，SG)
は
Mikolovらによって提案された単語の分散的意味表現手法．

これらのモデルを公開しているツール→word2vec


## CBOW

与えられた文脈の中で出現している文脈語を使って，ある対象語が出現しているかどうかを予測可能な意味表現を学習．

対象語 意味を表現したい単語
文脈語 ある単語の周辺に現れる単語

文s = 「私はご飯と味噌汁を食べた．」について，

形態素は，[私，は，ご飯，と，味噌汁，を，食べた]

対象語を「ご飯」とすると，それ以外の語が文脈語として用い，「ご飯」の出現を予測．

CBOWモデルでは，文章中に[私，は，と，味噌汁，を，食べた]という文脈語が出現していた時に，
その文章中に「ご飯」という単語が出現するかどうか予測できるように，
それぞれの単語の意味表現ベクトルを更新することが目的．

1つの単語につき，d次元のベクトル

i番目の単語が対象語の場合，その単語の対象語ベクトル$\mbox{\boldmath $x$}_i$を使用．

i番目の単語が文脈語の場合，その単語の文脈語ベクトル$\mbox{\boldmath $z$}_i$を使用．

対象語xが文脈中のi番目の単語して出現している場合，xを中心とする(2k+1)個の単語からなる文脈
$(i-k), \cdots, (i-1), i, (i+1), \cdots, (i+k)$
を使って予測する問題を考える．

この文脈中に出現する文脈語についての文脈ベクトルを
$\mbox{\boldmath $z$}_{i-k}, \cdots, \mbox{\boldmath $z$}_{i-1}, \mbox{\boldmath $z$}_{i+1}, \cdots, \mbox{\boldmath $z$}_{i+k}$
とすると対象語の出現確率は，
$p(\mbox{\boldmath $x$}_i|\mbox{\boldmath $z$}_{i-k}, \cdots, \mbox{\boldmath $z$}_{i-1}, \mbox{\boldmath $z$}_{i+1}, \cdots, \mbox{\boldmath $z$}_{i+k})$
と表すことができる．


→kを大きくするとより広い範囲で単語の共起が考慮可能．関係が低い離れた単語同士の共起も考慮されてしまう

→kはCBOWのハイパーパラメータ．Mikolovらは$k=2$として5単語からなる文脈窓を用いて学習

問題:長い連続する単語列の出現は大きなコーパスについても少ない→その出現確率を推定することは難しい

CBOWにおける解決策:文脈語の出現順序を無視，下式で与えられる文脈語ベクトルの平均ベクトル$\hat{\mbox{\boldmath $x$}}$を，対象語xの文脈を代表するベクトルとして用いる．

\[
\hat{\mbox{\boldmath $x$}} = \frac{1}{2k}(\mbox{\boldmath $z$}_{i-k} + \cdots + \mbox{\boldmath $z$}_{i-1} + \mbox{\boldmath $z$}_{i+1} + \cdots + \mbox{\boldmath $z$}{i+k})
\]

$\mbox{\boldmath $x$}$と$\hat{\mbox{\boldmath $x$}}$を用いて，対象語の出現確率をソフトマックス関数で以下のように計算

\[
p(\mbox{\boldmath $x$}_i|\mbox{\boldmath $z$}_{i-k}, \cdots, \mbox{\boldmath $z$}_{i-1}, \mbox{\boldmath $z$}_{i+1}, \cdots, \mbox{\boldmath $z$}{i+k}) = \frac{\exp(\hat{\mbox{\boldmath $x$}}^{\mathrm{T}} \mbox{\boldmath $x$})}{\sum_{x' \in \mbox{\boldmath $\nu$}}\exp(\hat{\mbox{\boldmath $x$}}^{\mathrm{T}} \mbox{\boldmath $x'$})}
\]

$\mbox{\boldmath $\nu$}$はコーパス中の全単語からなる語彙集合，$\mbox{\boldmath $x'$}$は$\mbox{\boldmath $\nu$}$中の単語を表す

xが文章中に出現する可能性が高いほど右辺・分子の内積の値が大きくする

1つの単語について2つのベクトルが付与される(文脈語ベクトルと対象語ベクトル)→単語の共起をベクトルの内積で表現しているから

1つじゃダメなの？→1つの場合，$\hat{\mbox{\boldmath $x$}}$の中にもxが現れることになり，$\mbox{\boldmath $x$}^{\mathrm{T}} x$を小さくすることになる．

→xの長さ(l2ノルム)を小さくすることになる

→xのl2ノルムを変えてもソフトマックス関数はスケール不変

→xをそのl2ノルムで割って正規化しても
$p(x_i|\mbox{\boldmath $z$}_{i-k}, \cdots, \mbox{\boldmath $z$}_{i-1}, \mbox{\boldmath $z$}_{i+1}, \cdots, \mbox{\boldmath $z$}{i+k})$
の値は変わらない

→異なる2つのベクトルを使うことでこの問題を解決

tips:単語の共起頻度を離散的な数ではなく，連続な値(ベクトルの内積)でモデル化しているので「continuous」

上記では出現順序を無視し，文脈語の平均を使って文脈を表現したが，出現順序を保つベクトルを作成することも可能

→文脈語のベクトルを連結する


## SG

対象語を使って，文脈に出現している文脈語を予測する


対象語の「ご飯」を持ちいて，他の単語([私，は，と，味噌汁，を，食べた])の出現を予測

対象語xがi番目の単語として出現している文脈で他の単語を同時に予測する場合の確率を下式で計算．

\[
p(z_{i-k}, \cdots, z_{i-1}, z_{i+1}, \cdots, z{i+k}|x) = p(z_{i-k}|x)\cdots p(z_{i-1}|x)p(z_{i+1}|x)\cdotsp(z_{i+k}|x)
\]

※対象語xが与えられているとき，文脈語が全て独立

独立性の仮定により，
\[
p(z|x) = \frac{\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})}{\sum_{z' \in \mbox{\boldmath $\nu$} (x)}\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z'$})}
\]

(→「わかりやすいパターン認識」など参照のこと)

$\mbox{\boldmath $\nu$} (x)$は対象語xが出現している文脈中での文脈語の集合．ある特定の文脈でxと共起する文脈語の集合ではなく，コーパス全体におけるxが出現する全ての文脈に関しての文脈語の集合

CBOWと同様文脈として，対象語xを中心とするk個の単語からなる文脈窓を選ぶことも可能

→kを固定せずに，それぞれのxの出現においてkとしてある区間内のランダム値を取るなど，文脈窓の幅を動的に決めることも可能

n個の単語それぞれについて，d次元の対象語ベクトルを学習

それぞれの単語について何らかの文脈で共起する文脈語が$\mbox{\boldmath $\nu$} (x)$なので，その単語全てについてd次元の文脈語ベクトルを学習


---

CBOWはSGに比べ，文脈語の独立性を近似していないため，より正確

1対1の共起をモデル化しているSGに比べ，CBOWは1対複数の文脈語をモデル化しているので，複数の文脈語からなる単語列がコーパス中に十分な回数現れなければならない

→CBOWはより大きなコーパスが必要


## CBOWとSGのモデル最適化

SGのモデルについて，

\[
p(z|x) = \frac{\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})}{\sum_{z' \in \mbox{\boldmath $\nu$} (x)}\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z'$})}
\]

xまたはzについて凸関数ではない→局所解が存在

xのある要素に注目するとxとzの内積はその要素に対して線形な関数となる

xかzのどちらかを固定した場合，片方の関数について上式は凸関数となる→双線形関数，交互最適化可能

さらに，上式は\expを含むので対数をとることで対数双線形関数

xかzを固定すると片方の変数に関してソフトマックス関数→多クラスロジスティック回帰としてみなせる



## 負例サンプリング

SGのモデルについて，単語の分散的意味表現を学習するために，
ある単語と共起する単語(正例)だけでなく，
共起しない単語(負例)に関する情報も必要

→共起しない単語集合は膨大．その中から学習のために「良い負例」を選択しなければいけない

→この負例手法のことを負例サンプリング(Negative Sampling)

### SGのNegative Sampling

ある単語xの分散的意味表現$\mbox{\boldmath $x$}$を求める問題を考える

正例として扱える文脈語の集合を$\mbox{\boldmath $D$}_{pos} = \mbox{\boldmath $\nu$} (x)$とし，負例の集合を$\mbox{\boldmath $D$}_{neg} \in \mbox{\boldmath $\nu$}$とする

それぞれの定義より，$\mbox{\boldmath $D$}_{pos} \cup \mbox{\boldmath $D$}_{neg} = \phi$

この時，SGモデルで定義される共起の予測確率に従い，
対数尤度を最大化するxの対象語ベクトルは下式で与えられる

\[
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\hat{\mbox{\boldmath $x$}} = \argmax_{x} (\sum_{z \in \mbox{\boldmath $D$}_{pos}} \log(p(t=1|\mbox{\boldmath $x$} ,\mbox{\boldmath $z$})p_{pos}(z|x)) + \sum_{z \in \mbox{\boldmath $D$}_{neg}} \log(p(t=-1|\mbox{\boldmath $x$} ,\mbox{\boldmath $z$})p_{neg}(z|x)))
\]


右辺第1項は対象語xと共起する文脈語zに関する対数尤度を表す．
その項の$t=1$は正例であることを示す．

$p_{pos}(z|x)$はxの正例集合$\mbox{\boldmath $D$}_{pos}$での出現確率を表しており，
正例集合はコーパス中に出現している単語集合$\mbox{\boldmath $\nu$} (x)$なので，
$\sum_{z \in \mbox{\boldmath $D$}_{pos}} p_{pos}(z|x) = 1$となる

右辺第2項について，負例$\mbox{\boldmath $\nu$}$はランダムにサンプリングしているためサンプリング手法に依存する．

本来なら負例は正例を除いた$\mbox{\boldmath $\nu$}$からサンプリングすべきだが，$\mbox{\boldmath $\nu$}$に比べ，$\mbox{\boldmath $D$}_{pos}$は極めて小さいため近似的に$\mbox{\boldmath $D$}_{neg}$を$\mbox{\boldmath $\nu$}$からサンプリングしている

→どの対象語xについても同一分布が使える利点

サンプリング手法は？

→SGではコーパス中で高出現確率である単語zが単語xの意味表現学習の負例として選ばれるように，単語zのユニグラム分布$p(z)$に基づいて負例集合を選択

負例が(x, z)のペアで決まるのなら，ユニグラム分布ではなく，xとzの同時分布からサンプリングすべきでは？

→2単語の同時共起確率に比べ，1単語の出現確率がゼロになりにくいという利点．

→膨大なコーパスから共起を計算するには大きな共起表を作らなければならず，空間計算量という点で好ましくない


単語の出現はZipfの法則に従うので$p(z)$はロングテールの分布となる．

→$p(z)$を3/4乗した値に比例する分布をサンプリング分布として用いている．

→出現確率$p(z)$が小さい文脈語zも比較的高頻度でサンプリングされるようになる

以上より，


\[
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\argmax_{x} (\sum_{z \in \mbox{\boldmath $D$}_{pos}} \log(p(t=1|\mbox{\boldmath $x$} ,\mbox{\boldmath $z$})) + \sum_{z \in \mbox{\boldmath $D$}_{neg}} \log((1 - p(t=1|\mbox{\boldmath $x$} ,\mbox{\boldmath $z$}))\tilde{p}(z))) = \argmax_{x} (\sum_{z \in \mbox{\boldmath $D$}_{pos}} \log(\sigma (\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})) + \sum_{z \in \mbox{\boldmath $D$}_{neg}} \log(\sigma (-\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})\tilde{p}(z)))
\]

$\sigma(\theta)$はロジスティックシグモイド関数

第2項の負例に関するサンプリング分布$\tilde{p}(z)$はxに無関係．
よって，第2項を期待値の形で書き表すことが可能

\[
\sum_{z \in \mbox{\boldmath $D$}_{pos}} \log(\sigma (\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})) + E_{\tlide{p}(z)} [ \log(\sigma (-\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})) ]
\]


1つの正例に対して，k個の負例を用いる．

負例はランダムに選択している擬似負例のため正例数に比べ，負例数を大きくしなければならない．

実用的には$k=20$．



word2vecに実装されているCBOWとSGでは，
非同期パラメータ更新による並列処理を行うことで学習時間の短縮

→複数のスレッドを用いて，同一学習モデルを更新していく．

→同じ学習モデルを更新すると互いに更新する値を打ち消しあう可能性，必ずしも正しく学習が行えるわけではない

より正確にするには

→オンライン学習の並列化でよく用いられるように反復的パラメータ混合方

→スレッドごとに独立にモデルを持たせて，独立的に学習させたモデルを足し合わせる


学習の正確さ トレードオフ 大量のデータから短時間で学習



## 階層型ソフトマックスによる近似計算

大規模なコーパスについて，多数の対象語と文脈語に関してベクトルを計算しなければならず，
高速に計算できることが望まれる．

\[
p(z|x) = \frac{\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})}{\sum_{z' \in \mbox{\boldmath $\nu$} (x)}\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z'$})}
\]

右辺分母は全文脈語彙集合にわたって規格化しなければならず，この計算に時間がかかる

解決策

→単語の階層構造を事前に事前に用意し，それに従って規格化を行う→階層型ソフトマックス

→単語の出現を考慮する代わりに，その単語を含むグループを考慮できるので，
式中の和の計算を行う際に，考慮すべき語彙集合を小さくできる

ある単語xがある1つのグループにのみ含まれるようにグループ分けされていれば，
全単語集合ではなく，このグループ内でのみxの確率を規格化しておけば良い

言語モデル構築の際に，単語そのものの出現頻度の代わりに，単語グループの出現頻度を使うことで出現頻度が少ない単語の出現確率を求める手法から発想を得ている



単語の階層構造をどのようにして作るか

1. 階層的クラスタリングで全ての単語を含む階層構造を作成
2. すでに構築されている概念構造を使う
  -  WordNet→単語を語義ごとにグループ化
    - グループ間で上位下位関係が記述されている



階層的クラスタリングとWordNetを使って大規模なコーパスから単語の分散的意味表現を学習する際の問題点

- 階層的クラスタリングの場合

  1. 単語と単語間の類似度とグループとグループ間の類似度を計算しなければならないため，計算時間がかかる

  2. どのような類似度尺度を使うべきかが明確ではない
    - 階層的クラスタリングによって作られる階層構造が，類似度尺度によって変わる

- WordNetの場合

  - WordNetに登録されていない単語をどのように分類するか


word2vecではハフマン木を使うことで単語の階層構造を構築

→単語の出現頻度を計算し，出現頻度の高い順にソート

→階層的クラスタリングよりも計算量の点で有利

→コーパス中の全ての単語がハフマン木のノードで表されているので未知語の問題が起きない



単語の階層構造を用いて，どのようにして確率$P(z|x)$を計算するか

ハフマン木上で単語zに対する頂点を見つけ，根からその頂点への経路Path(z)を求める

各ノードでは-1もしくは1という2つの枝があるため，$p(z|x)$の予測確率をロジスティック関数を使って，次のように近似できる

\begin{eqnarray*}
p(z|x) &=& \frac{\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z$})}{\sum_{z' \in \mbox{\boldmath $\nu$} (x)}\exp(\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $z'$})}\\
& \approx & \prod_{(l,y) \in Path(z)} p(t=l|\mbox{\boldmath $x$},\mbox{\boldmath $y$})\\
&=& \prod_{(l,y) \in Path(z)} \sigma(l\mbox{\boldmath $x$}^{\mathrm{T}} \mbox{\boldmath $y$})
\end{eqnarray*}

yは根から単語zに対する葉への経路上のノードを表し，lは各ノードでの枝の値(1, -1)を表し，
このlに対応する確率変数がt

ハフマン木は単語の出現頻度だけを使って構築されているので，
単語の意味的関係を反映しているとは言えないが元の式を近似するには十分な階層構造である．



## 大域ベクトル予測モデル

CBOWとSGはコーパスを一文単位で処理し，単語毎の2つのベクトルを更新している

→単語の共起を予測する際に，一文に含まれている情報しか使うことができない

→分散的意味表現ベクトルを学習する際に一文中の共起共起情報ではなく，
分布的意味表現のように，コーパス全体における2つの単語共起情報を使う手法

→大域ベクトル予測モデル(global vector prediction，GloVe)

分布的意味表現のように，コーパス全体における対象語と文脈語のコーパス全体における共起頻度を計算し，
共起行列$\mbox{\boldmath $X$}$を作成

$\mbox{\boldmath $X$}$の各行が対象語・各列が文脈語に対応しているとする

→i番目の対象語iとj番目の文脈語jが共起する確率回数を$\mbox{\boldmath $X$}$の(i,j)番目の要素$\mbox{\boldmath $X$}_{ij}$

次の目的関数を最小化する対象語ベクトル$\mbox{\boldmath $x$}_i$と文脈語ベクトル$\mbox{\boldmath $z$}_j$を学習

\[
J = \sum_{i,j=1}^{|\mbox{\boldmath $\nu$}|}f(X_{ij})(\mbox{\boldmath $x$}_i^{\mathrm{T}}\mbox{\boldmath $z$}_j + b_i + b_j - \log(X_{ij}))^2
\]

$b_i$と$b_j$はスカラーのバイアス項

→対象語ベクトルと文脈語ベクトルの内積を用いて，この共起頻度の対数を予測

関数fは次の3つの性質を満たす関数

1. $f(0)=0$
  - $x \to 0$のとき$\lim_{x \to 0} f(x)\log^2(x)$が有限の値を持つ．
    - 共起頻度が0であるペアに関してもJが無限大に発散してしまうのを防ぐ

2. $f(x)$は単調増加関数．
  - 低共起頻度のペアを過剰評価するのを防ぐ

3. $f(x)$はある閾値以上の共起頻度に関しては比較的小さな値を持つ
  - 不要語など高頻出する単語との共起を過剰評価するのを防ぐ

GloVeは$f$に下式を採用

\[
f(t) = \begin{cases}
  (t/t_{max})^\alpha & (t < t_{max}) \\
  1 & (otherwise)
\end{cases}
\]

Jは$\mbox{\boldmath $x$}_i$あるいは$\mbox{\boldmath $z$}_j$のどちらか一方を固定させた場合，
片方に対して線形であり，双線形関数．

→交互最適化手法



---

- GloVe → 目的関数が二乗誤差
- CBOW SG → 交差エントロピー誤差


GloVeがCBOW SGと大きく異なる点→負例を必要としない

→負例サンプリングを行うための様々な仮定や近似が不要になる

n個の単語の共起情報を保存するためには{n(n-1)/2 - n}個の変数(自分自身の共起は計算しないので-n)が必要となる

→空間情報量としては$O(n^2)$のオーダーの変数が必要

→共起行列をどのように計算するかがGloVeを用いて学習するための重要な前処理タスク

→実際どうするか

1. 分散処理
2. コーパス全体における各単語の出現頻度を求め，頻度が小さいものは共起行列に含まないようにする．
  - 出現頻度の低いもの→スペルミスの可能性
  - 出現頻度が低いと共起の可能性も低い→信頼できる統計情報が得られない可能性
  - 共起行列を構築するために最低2回コーパスを処理しなければならない

→CBOWとSGは一文単位で学習できるのでオンライン学習が可能
